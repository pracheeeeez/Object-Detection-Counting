{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSPB178vz9z4",
        "outputId": "bc98d69c-6516-4c18-e9d1-de6aa0d30229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-vc79v1_9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-vc79v1_9\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: segment_anything\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36592 sha256=91aba53983a20396071c80da345e3b47889dc719f8f287b24502bbb903cada7b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5nr146rj/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "Successfully built segment_anything\n",
            "Installing collected packages: segment_anything\n",
            "Successfully installed segment_anything-1.0\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam_checkpoint = \"/content/drive/MyDrive/sam_vit_h_4b8939.pth\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "sam_model = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "sam_model.to(device)\n",
        "sam_predictor = SamPredictor(sam_model)\n",
        "\n",
        "print(\"SAM Model Loaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_lMiKm20hTh",
        "outputId": "ac4c9420-7705-484d-f5c3-005c309102e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAM Model Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO_PATH = \"/content/17397133-trimmed.mp4\"\n",
        "TARGET_VIDEO_PATH = \"/content/output_video.mp4\"\n"
      ],
      "metadata": {
        "id": "HfrN1w9X4Yp_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import time\n",
        "import torch\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam_model)\n",
        "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter(TARGET_VIDEO_PATH, fourcc, frame_rate, (frame_width, frame_height))\n",
        "\n"
      ],
      "metadata": {
        "id": "c2E4H_NB4vYG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "frame_count = 0\n",
        "total_object_count = 0\n",
        "frame_skip = 5\n",
        "\n",
        "tracked_objects = {}\n",
        "line_y = frame_height - 100\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    if frame_count % frame_skip != 0:\n",
        "        frame_count += 1\n",
        "        continue\n",
        "\n",
        "\n",
        "    frame_resized = cv2.resize(frame, (frame_width // 2, frame_height // 2))\n",
        "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    masks = mask_generator.generate(frame_rgb)\n",
        "    object_count = 0\n",
        "    for mask in masks:\n",
        "        mask_image = mask[\"segmentation\"].astype(np.uint8)\n",
        "        if np.any(mask_image > 0):\n",
        "            object_count += 1\n",
        "            mask_image_resized = cv2.resize(mask_image, (frame_width, frame_height), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            frame[mask_image_resized > 0] = (0, 255, 0)\n",
        "    total_object_count += object_count\n",
        "    cv2.putText(frame, f\"Total Objects: {total_object_count}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "    cv2.line(frame, (0, line_y), (frame_width, line_y), (255, 0, 0), 2)\n",
        "    out.write(frame)\n",
        "    print(f\"Processed frame {frame_count + 1}, Total Objects: {total_object_count}\")\n",
        "    frame_count += 1\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Total objects counted across all frames: {total_object_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUWNdti09ueu",
        "outputId": "a77a30c0-d50c-4f16-b258-6ab8be62ac5c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed frame 1, Total Objects: 85\n",
            "Processed frame 6, Total Objects: 171\n",
            "Processed frame 11, Total Objects: 256\n",
            "Processed frame 16, Total Objects: 346\n",
            "Processed frame 21, Total Objects: 435\n",
            "Processed frame 26, Total Objects: 525\n",
            "Processed frame 31, Total Objects: 607\n",
            "Processed frame 36, Total Objects: 694\n",
            "Processed frame 41, Total Objects: 783\n",
            "Processed frame 46, Total Objects: 864\n",
            "Processed frame 51, Total Objects: 948\n",
            "Processed frame 56, Total Objects: 1029\n",
            "Processed frame 61, Total Objects: 1109\n",
            "Processed frame 66, Total Objects: 1187\n",
            "Processed frame 71, Total Objects: 1261\n",
            "Processed frame 76, Total Objects: 1338\n",
            "Processed frame 81, Total Objects: 1421\n",
            "Processed frame 86, Total Objects: 1504\n",
            "Processed frame 91, Total Objects: 1586\n",
            "Processed frame 96, Total Objects: 1673\n",
            "Processed frame 101, Total Objects: 1761\n",
            "Processed frame 106, Total Objects: 1846\n",
            "Processed frame 111, Total Objects: 1935\n",
            "Processed frame 116, Total Objects: 2021\n",
            "Processed frame 121, Total Objects: 2113\n",
            "Processed frame 126, Total Objects: 2204\n",
            "Processed frame 131, Total Objects: 2283\n",
            "Processed frame 136, Total Objects: 2356\n",
            "Total objects counted across all frames: 2356\n"
          ]
        }
      ]
    }
  ]
}